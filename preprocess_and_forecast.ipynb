{"cells":[{"cell_type":"code","source":["import gc\nimport os\n# import pickle\nimport numpy as np \nimport pandas as pd \nimport category_encoders as ce\nfrom category_encoders.ordinal import OrdinalEncoder\n# import os\nimport copy\nimport time\n# import numba\nimport lightgbm as lgb\n#import click\n#import tsforest \n#from tsforest.forecast import LightGBMForecaster\nfrom tsforest.utils import make_time_range\nfrom scipy.stats import trim_mean\n# import plotly.graph_objects as go \nimport matplotlib.pyplot as plt\n#from tsforest.forecast import LightGBMForecaster\n#from mahts import HTSDistributor\n#import bayes_opt"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:158: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  original_result = python_builtin_import(name, globals, locals, fromlist, level)\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["import sys\n#sys.path.append('/dbfs/Chronos/data/m5/')"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#os.listdir(\"/dbfs/Chronos/data/m5/\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import os\n# import pickle\nimport numpy as np \nimport pandas as pd \nimport category_encoders as ce\nfrom category_encoders.ordinal import OrdinalEncoder"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["\n## Read data ##\nos.listdir(\"/dbfs/Chronos/data/m5\")\nos.chdir(\"/dbfs/Chronos/data/m5\")\ncalendar = pd.read_csv('calendar.csv')\nsell_prices = pd.read_csv('sell_prices.csv')\nsales_train = pd.read_csv('sales_train_evaluation.csv')\nsales_train[\"id\"] = sales_train.id.map(lambda x: x.replace(\"_evaluation\", \"\"))"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["sales_train[\"item_no\"] = sales_train[\"item_id\"].apply(lambda x: int(x[-3:]))\nsales_train = sales_train[sales_train[\"item_no\"].isin(range(1, 600,2))]\nsales_train = sales_train.drop(\"item_no\", axis = 1)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# ## Create Subset ##\n# product = sales_train[['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n# product[\"item_no\"] = product[\"item_id\"].apply(lambda x: x[-3:])\n# product[\"item_no\"] = product[\"item_no\"].apply(lambda x: int(x)) ## Insgesamt 827 unique item_no\n# product = product[product.cat_id.isin(['HOBBIES','FOODS'])]\n\n# product_subset = product[product[\"item_no\"] < 80] \n# product_subset.drop(\"item_no\",axis=1,inplace=True)\n# keys = list(product_subset.columns.values)\n# i1 = sales_train.set_index(keys).index\n# i2 = product_subset.set_index(keys).index\n# sales_train = sales_train[i1.isin(i2)]"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["## Hierarchical Encoding ##\nhierarchy_raw = (sales_train.loc[:, [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]\n             .drop_duplicates())\nencoders = dict()\nhierarchy = hierarchy_raw.copy()\n\nid_encoder = OrdinalEncoder()\nid_encoder.fit(hierarchy.loc[:, [\"id\"]])\nhierarchy[\"ts_id\"]  = id_encoder.transform(hierarchy.loc[:, [\"id\"]])\nencoders[\"id\"] = id_encoder\n\nitem_encoder = OrdinalEncoder()\nitem_encoder.fit(hierarchy.loc[:, [\"item_id\"]])\nhierarchy.loc[:, \"item_id\"]  = item_encoder.transform(hierarchy.loc[:, [\"item_id\"]])\nencoders[\"item\"] = item_encoder\n\ndept_encoder = OrdinalEncoder()\ndept_encoder.fit(hierarchy.loc[:, [\"dept_id\"]])\nhierarchy.loc[:, \"dept_id\"]  = dept_encoder.transform(hierarchy.loc[:, [\"dept_id\"]])\nencoders[\"dept\"] = dept_encoder\n\ncat_encoder = OrdinalEncoder()\ncat_encoder.fit(hierarchy.loc[:, [\"cat_id\"]])\nhierarchy.loc[:, \"cat_id\"]   = cat_encoder.transform(hierarchy.loc[:, [\"cat_id\"]])\nencoders[\"cat\"] = cat_encoder\n\nstore_encoder = OrdinalEncoder()\nstore_encoder.fit(hierarchy.loc[:, [\"store_id\"]])\nhierarchy.loc[:, \"store_id\"] = store_encoder.transform(hierarchy.loc[:, [\"store_id\"]])\nencoders[\"store\"] = store_encoder\n\nstate_encoder = OrdinalEncoder()\nstate_encoder.fit(hierarchy.loc[:, [\"state_id\"]])\nhierarchy.loc[:, \"state_id\"] = state_encoder.transform(hierarchy.loc[:, [\"state_id\"]])\nencoders[\"state\"] = state_encoder\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["## Encode Calendar Events ##\nevent_name_1_encoder = OrdinalEncoder()\nevent_name_1_encoder.fit(calendar.loc[:, [\"event_name_1\"]])\ncalendar.loc[:, \"event_name_1\"] = event_name_1_encoder.transform(calendar.loc[:, [\"event_name_1\"]])\n\nevent_type_1_encoder = OrdinalEncoder()\nevent_type_1_encoder.fit(calendar.loc[:, [\"event_type_1\"]])\ncalendar.loc[:, \"event_type_1\"] = event_type_1_encoder.transform(calendar.loc[:, [\"event_type_1\"]])\n\nevent_name_2_encoder = OrdinalEncoder()\nevent_name_2_encoder.fit(calendar.loc[:, [\"event_name_2\"]])\ncalendar.loc[:, \"event_name_2\"] = event_name_2_encoder.transform(calendar.loc[:, [\"event_name_2\"]])\n\nevent_type_2_encoder = OrdinalEncoder()\nevent_type_2_encoder.fit(calendar.loc[:, [\"event_type_2\"]])\ncalendar.loc[:, \"event_type_2\"] = event_type_2_encoder.transform(calendar.loc[:, [\"event_type_2\"]])"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["## Enocde categorical features ##\n\nsales_train.loc[:, \"item_id\"]  = item_encoder.transform(sales_train.loc[:, [\"item_id\"]])\nsales_train.loc[:, \"dept_id\"]  = dept_encoder.transform(sales_train.loc[:, [\"dept_id\"]])\nsales_train.loc[:, \"cat_id\"]   = cat_encoder.transform(sales_train.loc[:, [\"cat_id\"]])\nsales_train.loc[:, \"store_id\"] = store_encoder.transform(sales_train.loc[:, [\"store_id\"]])\nsales_train.loc[:, \"state_id\"] = state_encoder.transform(sales_train.loc[:, [\"state_id\"]])\n\n## Encode features in sell_prices ##\nsell_prices.loc[:, \"store_id\"] = store_encoder.transform(sell_prices.loc[:, [\"store_id\"]])\nsell_prices.loc[:, \"item_id\"]  = item_encoder.transform(sell_prices.loc[:, [\"item_id\"]]) "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["actual_data = pd.melt(sales_train, \n               id_vars=[\"ts_id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"],\n               value_vars=[f\"d_{i}\" for i in range(1913,1942)],\n               var_name=\"d\",\n               value_name=\"q\")\nactual_data = actual_data.merge(calendar[[\"d\",\"date\"]], on = \"d\").drop(\"d\", axis = 1)\nactual_data[\"date\"] = pd.to_datetime(actual_data[\"date\"])"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["## Melt and Merge Datasets ##\ndata = pd.melt(sales_train, \n               id_vars=[\"ts_id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"],\n               value_vars=[f\"d_{i}\" for i in range(1,1913)],\n               var_name=\"d\",\n               value_name=\"q\")\n\ncalendar_columns = [\"date\", \"wm_yr_wk\", \"d\", \"snap_CA\", \"snap_TX\", \"snap_WI\",\n                    \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n\ndata = pd.merge(data, \n                calendar.loc[:, calendar_columns],\n                how=\"left\",\n                on=\"d\")\n\ndata = pd.merge(data, sell_prices,\n                on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],\n                how=\"left\")\n\ndata.sort_values([\"item_id\",\"store_id\",\"date\"], inplace=True, ignore_index=True)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["data.drop([\"item_id\"],axis=1,inplace=True)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["data[\"revenue\"] = data[\"sell_price\"]*data[\"q\"]"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["\nsales_train_agg = data.groupby([\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"wm_yr_wk\",\"snap_CA\",\"snap_TX\",\"snap_WI\",\"event_name_1\",\"event_type_1\",\"event_name_2\",\"event_type_2\",\"date\"],as_index=False)[\"revenue\"].sum()"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["sales_train_agg"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[82]: </div>"]}},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dept_id</th>\n      <th>cat_id</th>\n      <th>store_id</th>\n      <th>state_id</th>\n      <th>wm_yr_wk</th>\n      <th>snap_CA</th>\n      <th>snap_TX</th>\n      <th>snap_WI</th>\n      <th>event_name_1</th>\n      <th>event_type_1</th>\n      <th>event_name_2</th>\n      <th>event_type_2</th>\n      <th>date</th>\n      <th>revenue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11101</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2011-01-29</td>\n      <td>1276.86</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11101</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2011-01-30</td>\n      <td>874.41</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11101</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2011-01-31</td>\n      <td>917.82</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11101</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2011-02-04</td>\n      <td>906.72</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11101</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2011-02-02</td>\n      <td>665.08</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>133835</th>\n      <td>7</td>\n      <td>3</td>\n      <td>10</td>\n      <td>3</td>\n      <td>11612</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2016-04-19</td>\n      <td>3935.21</td>\n    </tr>\n    <tr>\n      <th>133836</th>\n      <td>7</td>\n      <td>3</td>\n      <td>10</td>\n      <td>3</td>\n      <td>11612</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2016-04-20</td>\n      <td>3714.63</td>\n    </tr>\n    <tr>\n      <th>133837</th>\n      <td>7</td>\n      <td>3</td>\n      <td>10</td>\n      <td>3</td>\n      <td>11612</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2016-04-21</td>\n      <td>3554.51</td>\n    </tr>\n    <tr>\n      <th>133838</th>\n      <td>7</td>\n      <td>3</td>\n      <td>10</td>\n      <td>3</td>\n      <td>11612</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2016-04-22</td>\n      <td>4219.22</td>\n    </tr>\n    <tr>\n      <th>133839</th>\n      <td>7</td>\n      <td>3</td>\n      <td>10</td>\n      <td>3</td>\n      <td>11613</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2016-04-23</td>\n      <td>5039.09</td>\n    </tr>\n  </tbody>\n</table>\n<p>133840 rows × 14 columns</p>\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["#sales_train_agg = pd.DataFrame(sales_train_agg).reset_index(inplace=True)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["sales_train_agg[\"combine\"] =sales_train_agg[\"dept_id\"].astype(str)+\"_\"+sales_train_agg[\"cat_id\"].astype(str)+\"_\"+sales_train_agg[\"store_id\"].astype(str)+\"_\"+sales_train_agg[\"state_id\"].astype(str)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["#sales_train_agg[\"ts_id\"] = id_encoder.transform(sales_train_agg.loc[:, [\"combine\"]])\ndata=sales_train_agg"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["## Encode snap-features ##\ndata[\"snap\"] = 0\n\nidx_snap_ca = data.query(\"state_id==1 & snap_CA==1\").index\ndata.loc[idx_snap_ca, \"snap\"] = 1\n\nidx_snap_tx = data.query(\"state_id==2 & snap_TX==1\").index\ndata.loc[idx_snap_tx, \"snap\"] = 2\n\nidx_snap_wi = data.query(\"state_id==3 & snap_WI==1\").index\ndata.loc[idx_snap_wi, \"snap\"] = 3\n\ndata.drop([\"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1, inplace=True)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["\ndef reduce_mem_usage(df, verbose=False):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n#data = reduce_mem_usage(data)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["def remove_starting_zeros(dataframe):\n    idxmin = dataframe.query(\"revenue > 0\").index.min()\n    return dataframe.loc[idxmin:, :]\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["data = (data\n        .groupby([\"combine\"])\n        .apply(remove_starting_zeros)\n        .reset_index(drop=True)\n       )"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["def find_out_of_stock(df):\n    df = df.copy()\n\n    df[\"no_stock_days\"] = 0\n    zero_mask = (df.revenue == 0)\n    transition_mask = (zero_mask != zero_mask.shift(1)) #TRUE, falls es von 0 zu !0 (oder umgekehrt) übergeht\n    zero_sequences = transition_mask.cumsum()[zero_mask] ## Aufsummieren aller TRUE (1) \n    zero_seqs_count = zero_sequences.map(zero_sequences.value_counts()).to_frame() ## Zähle wie häufig die jeweiligen kumulierten Werte auftreten\n    df.loc[zero_seqs_count.index, \"no_stock_days\"] = zero_seqs_count.revenue.values\n    return df\n\n## no_stock_data gibt an wie lange die Periode ist, für die das jeweilige Item im jeweiligen Store nicht verkauft wird. \n## Ist q für ein Item in eime Store für 4 Tage lang 0 (= es wird vier Tage lang nicht verkauft), so ist no_stock_data für \n## alle vier Zeitpunkte gleich 4 (siehe z.B. data.tail(20)) \ndata = data.groupby([\"combine\"]).apply(find_out_of_stock)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["data.reset_index(drop=True, inplace=True)\ndata.drop([ \"wm_yr_wk\"], axis=1, inplace=True)\ndata.rename({\"date\":\"ds\"}, axis=1, inplace=True)\ndata = data.rename({\"revenue\":\"y\"}, axis=1)\ndata[\"ds\"] = pd.to_datetime(data[\"ds\"])\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["## Create evaluation dataframe ##\ncalendar_columns = [\"date\", \"wm_yr_wk\", \"snap_CA\", \"snap_TX\", \"snap_WI\",\n                    \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\ncalendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n\neval_dataframe = (pd.concat([make_time_range(\"2016-04-24\", \"2016-05-22\", \"D\").assign(**row)\n                             for _,row in hierarchy.iterrows()], ignore_index=True)\n                  .merge(calendar.loc[:, calendar_columns],\n                         how=\"left\", left_on=\"ds\", right_on=\"date\")\n                  .merge(sell_prices, how=\"left\")\n                  .drop([\"id\",\"date\",\"wm_yr_wk\"], axis=1)\n                 )\n\neval_dataframe[\"snap\"] = 0\n\nidx_snap_ca = eval_dataframe.query(\"state_id==1 & snap_CA==1\").index\neval_dataframe.loc[idx_snap_ca, \"snap\"] = 1\n\nidx_snap_tx = eval_dataframe.query(\"state_id==2 & snap_TX==1\").index\neval_dataframe.loc[idx_snap_tx, \"snap\"] = 2\n\nidx_snap_wi = eval_dataframe.query(\"state_id==3 & snap_WI==1\").index\neval_dataframe.loc[idx_snap_wi, \"snap\"] = 3\n\neval_dataframe.drop([\"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1, inplace=True)\n\neval_dataframe[\"no_stock_days\"] = None\neval_dataframe[\"ds\"] = pd.to_datetime(eval_dataframe[\"ds\"])\n\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["id_encoder = OrdinalEncoder()\nid_encoder.fit(data.loc[:, [\"combine\"]])\ndata[\"ts_id\"]  = id_encoder.transform(data.loc[:, [\"combine\"]])"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["data[\"combine\"].unique()"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[102]: array([&#39;1_1_10_3&#39;, &#39;1_1_1_1&#39;, &#39;1_1_2_1&#39;, &#39;1_1_3_1&#39;, &#39;1_1_4_1&#39;, &#39;1_1_5_2&#39;,\n       &#39;1_1_6_2&#39;, &#39;1_1_7_2&#39;, &#39;1_1_8_3&#39;, &#39;1_1_9_3&#39;, &#39;2_1_10_3&#39;, &#39;2_1_1_1&#39;,\n       &#39;2_1_2_1&#39;, &#39;2_1_3_1&#39;, &#39;2_1_4_1&#39;, &#39;2_1_5_2&#39;, &#39;2_1_6_2&#39;, &#39;2_1_7_2&#39;,\n       &#39;2_1_8_3&#39;, &#39;2_1_9_3&#39;, &#39;3_2_10_3&#39;, &#39;3_2_1_1&#39;, &#39;3_2_2_1&#39;, &#39;3_2_3_1&#39;,\n       &#39;3_2_4_1&#39;, &#39;3_2_5_2&#39;, &#39;3_2_6_2&#39;, &#39;3_2_7_2&#39;, &#39;3_2_8_3&#39;, &#39;3_2_9_3&#39;,\n       &#39;4_2_10_3&#39;, &#39;4_2_1_1&#39;, &#39;4_2_2_1&#39;, &#39;4_2_3_1&#39;, &#39;4_2_4_1&#39;, &#39;4_2_5_2&#39;,\n       &#39;4_2_6_2&#39;, &#39;4_2_7_2&#39;, &#39;4_2_8_3&#39;, &#39;4_2_9_3&#39;, &#39;5_3_10_3&#39;, &#39;5_3_1_1&#39;,\n       &#39;5_3_2_1&#39;, &#39;5_3_3_1&#39;, &#39;5_3_4_1&#39;, &#39;5_3_5_2&#39;, &#39;5_3_6_2&#39;, &#39;5_3_7_2&#39;,\n       &#39;5_3_8_3&#39;, &#39;5_3_9_3&#39;, &#39;6_3_10_3&#39;, &#39;6_3_1_1&#39;, &#39;6_3_2_1&#39;, &#39;6_3_3_1&#39;,\n       &#39;6_3_4_1&#39;, &#39;6_3_5_2&#39;, &#39;6_3_6_2&#39;, &#39;6_3_7_2&#39;, &#39;6_3_8_3&#39;, &#39;6_3_9_3&#39;,\n       &#39;7_3_10_3&#39;, &#39;7_3_1_1&#39;, &#39;7_3_2_1&#39;, &#39;7_3_3_1&#39;, &#39;7_3_4_1&#39;, &#39;7_3_5_2&#39;,\n       &#39;7_3_6_2&#39;, &#39;7_3_7_2&#39;, &#39;7_3_8_3&#39;, &#39;7_3_9_3&#39;], dtype=object)</div>"]}}],"execution_count":30},{"cell_type":"code","source":["## Set model parameter ##\ndef compute_czeros(x):\n  return np.sum(np.cumprod((x==0)[::-1]))/x.shape[0]\n\ndef compute_sfreq(x):\n  return np.sum(x!=0)/x.shape[0]\n\napproach=1\n\nmodel_params = {\n  'objective':'tweedie',\n  'tweedie_variance_power': 1.1,\n  'metric':'None',\n  'max_bin': 127,\n  'bin_construct_sample_cnt':20000000,\n  'num_leaves': 2**10-1,\n  'min_data_in_leaf': 2**10-1,\n  'learning_rate': 0.05,\n  'feature_fraction':0.8,\n  'bagging_fraction':0.8,\n  'bagging_freq':1,\n  'lambda_l2':0.1,\n  'boost_from_average': False,\n}\n\ntime_features = [\n  \"year\",\n  \"month\",\n  #\"year_week\",\n  #\"year_day\",\n  \"week_day\",\n  \"month_progress\",\n  #\"week_day_cos\",\n  #\"week_day_sin\",\n  #\"year_day_cos\",\n  #\"year_day_sin\",\n  \"year_week_cos\",\n  \"year_week_sin\",\n  #\"month_cos\",\n  #\"month_sin\"\n]\n\nexclude_features = [\n  \"ts_id\",\n  \"no_stock_days\",\n  \"sales\",\n]\n\ncategorical_features = {\n  \"store_id\": \"default\",  ## Wird automatisch mit \"default\" hinzugefügt, da in ts_uid_columns\n  \"item_id\": \"default\", ## Wird automatisch mit \"default\" hinzugefügt, da in ts_uid_columns\n  \"state_id\": \"default\",\n  \"dept_id\": \"default\",\n  \"cat_id\": \"default\",\n  \"event_name_1\": \"default\",}\nif approach == 1:\n  categorical_features[\"item_id\"] = \"default\"\nelif approach == 2:\n  categorical_features[\"item_id\"] = (\"y\", ce.GLMMEncoder, None)\nelse:\n  print(\"Invalid input.\")\n\nmodel_kwargs = {\n  \"model_params\":model_params,\n  \"time_features\":['week_day', 'month_progress', 'year_week_cos', 'year_week_sin'],\n  \"window_functions\":{\n    \"mean\":   (None, [1,7,28], [7,14,28]),\n      \"std\":    (None, [1,7,28], [7,14,28]),\n    \"kurt\":   (None, [1,7,28], [7,28]),\n    \"czeros\": (compute_czeros, [1,], [7,14])\n    },\n  \"exclude_features\":exclude_features,\n  \"categorical_features\":categorical_features,\n  \"ts_uid_columns\":[\"item_id\",\"store_id\"],\n  #\"ts_uid_columns\": [\"store_id\"]\n}\n\nlagged_features_to_dropna = list()\nif \"lags\" in model_kwargs.keys():\n    lag_features = [f\"lag{lag}\" for lag in model_kwargs[\"lags\"]]\n    lagged_features_to_dropna.extend(lag_features)\nif \"window_functions\" in model_kwargs.keys():\n    rw_features = list()\n    for window_func,window_def in model_kwargs[\"window_functions\"].items():\n        _,window_shifts,window_sizes = window_def\n        if window_func in [\"mean\",\"median\",\"std\",\"min\",\"max\"]:\n            rw_features.extend([f\"{window_func}{window_size}_shift{window_shift}\"\n                                for window_size in window_sizes\n                                for window_shift in window_shifts])\n    lagged_features_to_dropna.extend(rw_features)\n\n# SEEDS = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, \n#          43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n# NUM_ITER_RANGE = (500,701)\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["import datatable as dt\ndata = dt.Frame(data)\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-430997492991477&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">import</span> datatable <span class=\"ansi-green-fg\">as</span> dt\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> data <span class=\"ansi-blue-fg\">=</span> dt<span class=\"ansi-blue-fg\">.</span>Frame<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    156</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    157</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 158</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;datatable&#39;</div>"]}}],"execution_count":32},{"cell_type":"code","source":["data.to_jay(\"daten.jay\")\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["#from tsforest.forecast import LightGBMForecaster"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-430997492991479&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> tsforest<span class=\"ansi-blue-fg\">.</span>forecast <span class=\"ansi-green-fg\">import</span> LightGBMForecaster\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    156</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    157</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 158</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/tsforest/forecast.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>                              CatBoostRegressor<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>                              XGBoostRegressor)\n<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> tsforest<span class=\"ansi-blue-fg\">.</span>forecast_base <span class=\"ansi-green-fg\">import</span> ForecasterBase\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-green-fg\">import</span> category_encoders <span class=\"ansi-green-fg\">as</span> ce\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> \n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    156</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    157</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 158</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/tsforest/forecast_base.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-green-fg\">from</span> inspect <span class=\"ansi-green-fg\">import</span> getmembers<span class=\"ansi-blue-fg\">,</span> isfunction\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> <span class=\"ansi-green-fg\">from</span> tsforest <span class=\"ansi-green-fg\">import</span> metrics\n<span class=\"ansi-green-fg\">----&gt; 8</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> tsforest<span class=\"ansi-blue-fg\">.</span>trend <span class=\"ansi-green-fg\">import</span> TrendModel\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> from tsforest.features import (compute_train_features, \n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span>                                compute_predict_features<span class=\"ansi-blue-fg\">,</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    156</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    157</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 158</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/tsforest/trend.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">import</span> pandas <span class=\"ansi-green-fg\">as</span> pd\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">from</span> fbprophet <span class=\"ansi-green-fg\">import</span> Prophet\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> stldecompose <span class=\"ansi-green-fg\">import</span> decompose<span class=\"ansi-blue-fg\">,</span> forecast\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-green-fg\">from</span> stldecompose<span class=\"ansi-blue-fg\">.</span>forecast_funcs <span class=\"ansi-green-fg\">import</span> naive<span class=\"ansi-blue-fg\">,</span> drift<span class=\"ansi-blue-fg\">,</span> mean<span class=\"ansi-blue-fg\">,</span> seasonal_naive\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-green-fg\">from</span> tsforest<span class=\"ansi-blue-fg\">.</span>config <span class=\"ansi-green-fg\">import</span> default_prophet_kwargs<span class=\"ansi-blue-fg\">,</span> default_stl_kwargs\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    156</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    157</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 158</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/stldecompose/__init__.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> <span class=\"ansi-blue-fg\">.</span>stl <span class=\"ansi-green-fg\">import</span> decompose<span class=\"ansi-blue-fg\">,</span> forecast\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    156</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    157</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 158</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/stldecompose/stl.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">from</span> pandas<span class=\"ansi-blue-fg\">.</span>core<span class=\"ansi-blue-fg\">.</span>nanops <span class=\"ansi-green-fg\">import</span> nanmean <span class=\"ansi-green-fg\">as</span> pd_nanmean\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> <span class=\"ansi-green-fg\">from</span> statsmodels<span class=\"ansi-blue-fg\">.</span>tsa<span class=\"ansi-blue-fg\">.</span>seasonal <span class=\"ansi-green-fg\">import</span> DecomposeResult\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> statsmodels<span class=\"ansi-blue-fg\">.</span>tsa<span class=\"ansi-blue-fg\">.</span>filters<span class=\"ansi-blue-fg\">.</span>_utils <span class=\"ansi-green-fg\">import</span> _maybe_get_pandas_wrapper_freq\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-green-fg\">import</span> statsmodels<span class=\"ansi-blue-fg\">.</span>api <span class=\"ansi-green-fg\">as</span> sm\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> \n\n<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;_maybe_get_pandas_wrapper_freq&#39; from &#39;statsmodels.tsa.filters._utils&#39; (/databricks/python/lib/python3.8/site-packages/statsmodels/tsa/filters/_utils.py)</div>"]}}],"execution_count":34},{"cell_type":"code","source":["## Create modell with model parameters and set features ##\ntic = time.time()\nmodel_level12_base = LightGBMForecaster(**model_kwargs) ## Erzeugen eines Objekts LightGBMForecaster, das von ForecasterBase erbt\nmodel_level12_base.prepare_features(train_data=data) ## Methode aus ForecasterBase\nmodel_level12_base.train_features.dropna(subset=lagged_features_to_dropna, axis=0, inplace=True) ## Methode aus ForecasterBase\n# model_level12_base.train_features = reduce_mem_usage(model_level12_base.train_features) ## Methode aus ForecasterBase\ngc.collect()\ntac = time.time()\nprint(f\"Elapsed time: {(tac-tic)/ 60.} [min]\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-430997492991480&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\">## Create modell with model parameters and set features ##</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> tic <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>model_level12_base <span class=\"ansi-blue-fg\">=</span> LightGBMForecaster<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">**</span>model_kwargs<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\">## Erzeugen eines Objekts LightGBMForecaster, das von ForecasterBase erbt</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> model_level12_base<span class=\"ansi-blue-fg\">.</span>prepare_features<span class=\"ansi-blue-fg\">(</span>train_data<span class=\"ansi-blue-fg\">=</span>data<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\">## Methode aus ForecasterBase</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> model_level12_base<span class=\"ansi-blue-fg\">.</span>train_features<span class=\"ansi-blue-fg\">.</span>dropna<span class=\"ansi-blue-fg\">(</span>subset<span class=\"ansi-blue-fg\">=</span>lagged_features_to_dropna<span class=\"ansi-blue-fg\">,</span> axis<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span> inplace<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\">## Methode aus ForecasterBase</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;LightGBMForecaster&#39; is not defined</div>"]}}],"execution_count":35},{"cell_type":"code","source":["## Fit the model ## \n# test = list()\n#for i,seed in enumerate(SEEDS[1]):   \n#num_iterations = np.random.randint(*NUM_ITER_RANGE)\nnum_iterations = 600\nseed = 3\n#print(\"#\"*100)\n# print(f\" model {i+1}/{len(SEEDS)} - seed: {seed} - num_iterations: {num_iterations} \".center(100, \"#\"))\n#print(\"#\"*100)\n\nmodel_level12 = copy.deepcopy(model_level12_base)\nmodel_params[\"seed\"] = seed\nmodel_params[\"num_iterations\"] = num_iterations\nmodel_level12.set_params(model_params)\n\nprint(\"Fitting the model\")\ntic = time.time()\nmodel_level12.fit()\ntac = time.time()\nprint(f\"Elapsed time: {(tac-tic)/60.} [min]\")"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["## Predict values ## \n# thresh_value\npredict_data = model_level12.train_features.query(\"no_stock_days >= 28\").loc[:, model_level12.input_features]\npredictions = model_level12.model.model.predict(predict_data) ## Aufrufen der predict methode von lightgbm.train\nprint(predictions)\nthresh_value = trim_mean(predictions, proportiontocut=0.05)\nprint(thresh_value)\ndef bias_corr_func(x, tv=thresh_value):\n    x[x < tv] = 0\n    return x\n\nprint(\"Predicting\")\ntic = time.time()\nforecast = model_level12.predict(eval_dataframe, recursive=True, bias_corr_func=bias_corr_func) ## Erzeugen einer rekursiven Vorhersage \n## Rekursiv beduetet hier, dass die lag-feature in Periode t abhängig von den in Periode t-1 berechneten lag-featuren ermittlet werden\nprint(forecast)\ntac = time.time()\n# test.append(forecast)\nprint(f\"Elapsed time: {(tac-tic)/60.} [min]\")\n"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["forecast_test = forecast.merge(actual_data[[\"ts_id\",\"item_id\", \"store_id\", \"date\"]], left_on = [\"ds\",\"item_id\", \"store_id\"], right_on = [\"date\",\"item_id\", \"store_id\"])"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["## Feature importance ##\n# item_id ist GLMM-Encoded und daher nicht mehr so relevant ##\nlgb.plot_importance(model_level12.model.model, importance_type=\"split\", figsize=(15,10))\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["## Forecast ohne trend correction und hierarchical reconciliation aggregiert (insgesamt) ##\ndata_agg = data.groupby([\"ds\"])[\"y\"].sum().reset_index()\ndata_act = actual_data.groupby([\"date\"])[\"q\"].sum().reset_index()\nforecast_agg = forecast.groupby([\"ds\"])[\"y_pred\"].sum().reset_index()\n\nplt.figure(figsize=(20,7))\nplt.plot_date(data_agg.ds, data_agg.y, \"o-\", label=\"historic\", color = \"lightblue\")\nplt.plot_date(data_act.date, data_act.q, \"o-\", label=\"original\", color = \"navy\")\nplt.plot_date(forecast_agg.ds, forecast_agg.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\nplt.grid()\nplt.legend(loc=\"best\")\nplt.show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# Forecast ohne trend correction und hierarchical reconciliation aggregiert (auf store_id) ##\ndata_agg_store = data.groupby([\"ds\", \"store_id\"])[\"y\"].sum().reset_index()\ndata_act_store = actual_data.groupby([\"date\", \"store_id\"])[\"q\"].sum().reset_index()\nforecast_agg_store = forecast.groupby([\"ds\", \"store_id\"])[\"y_pred\"].sum().reset_index()\n\nfor store_id in range(1,10):\n  data_agg_store_i = data_agg_store[data_agg_store[\"store_id\"] == store_id]\n  data_act_store_i = data_act_store[data_act_store[\"store_id\"] == store_id]\n  forecast_agg_store_i = forecast_agg_store[forecast_agg_store[\"store_id\"] == store_id]\n\n  plt.figure(figsize=(20,7))\n  plt.plot_date(data_agg_store_i.ds, data_agg_store_i.y, \"o-\", label=\"original\", color = \"lightblue\")\n  plt.plot_date(data_act_store_i.date, data_act_store_i.q, \"o-\", label=\"original\", color = \"navy\")\n  plt.plot_date(forecast_agg_store_i.ds, forecast_agg_store_i.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n  plt.grid()\n  plt.legend(loc=\"best\")\n  plt.show()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["## Forecast ohne trend correction und hierarchical reconciliation für item_id = 4# \n# data_item = data[data[\"item_id\"] == 4]\n# data_act_item = actual_data[actual_data[\"item_id\"] == 4]\n# forecast_item = forecast[forecast[\"item_id\"] == 4]\n\nfor i in range(1,10): \n  data_item = data[data[\"ts_id\"] == i]\n  data_act_item = actual_data[actual_data[\"ts_id\"] == i]\n  forecast_item = forecast_test[forecast_test[\"ts_id\"] == i]\n  plt.figure(figsize=(20,7))\n  plt.plot_date(data_item.ds, data_item.y, \"o-\", label=\"original\", color = \"lightblue\")\n  plt.plot_date(data_act_item.date, data_act_item.q, \"o-\", label=\"original\", color = \"navy\")\n  plt.plot_date(forecast_item.ds, forecast_item.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n  plt.grid()\n  plt.legend(loc=\"best\")\n  plt.show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["## Set parameters for trend correction model ##\nkwargs_list = [\n    # round 0\n    ({\"primary_bandwidths\": np.arange(41, 47),\n     \"middle_bandwidth\": 33,\n     \"final_bandwidth\": 15,\n     \"alpha\": 4,\n      \"drop_last_n\": 0},\n     {\"primary_bandwidths\": np.arange(112, 119),\n     \"middle_bandwidth\": 38,\n     \"final_bandwidth\": 33,\n     \"alpha\": 2}),\n    \n    ({\"primary_bandwidths\": np.arange(42, 46),\n     \"middle_bandwidth\": 19,\n     \"final_bandwidth\": 18,\n     \"alpha\": 4,\n      \"drop_last_n\": 0},\n     {\"primary_bandwidths\": np.arange(112, 119),\n     \"middle_bandwidth\": 30,\n     \"final_bandwidth\": 33,\n     \"alpha\": 1}),\n    \n    ({\"primary_bandwidths\": np.arange(42, 46),\n     \"middle_bandwidth\": 35,\n     \"final_bandwidth\": 15,\n     \"alpha\": 4,\n      \"drop_last_n\": 0},\n     {\"primary_bandwidths\": np.arange(112, 119),\n     \"middle_bandwidth\": 42,\n     \"final_bandwidth\": 33,\n     \"alpha\": 10}),\n    \n    # round 2\n    ({\"primary_bandwidths\": np.arange(20, 55),\n     \"middle_bandwidth\": 55,\n     \"final_bandwidth\": 38,\n     \"alpha\": 0,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(112, 119),\n     \"middle_bandwidth\": 43,\n     \"final_bandwidth\": 33,\n     \"alpha\": 0}),\n    \n    ({\"primary_bandwidths\": np.arange(21, 54),\n     \"middle_bandwidth\": 55,\n     \"final_bandwidth\": 41,\n     \"alpha\": 1,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(112, 119),\n     \"middle_bandwidth\": 41,\n     \"final_bandwidth\": 33,\n     \"alpha\": 0}),\n    \n    ({\"primary_bandwidths\": np.arange(24, 56),\n     \"middle_bandwidth\": 54,\n     \"final_bandwidth\": 41,\n     \"alpha\": 1,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(112, 119),\n     \"middle_bandwidth\": 31,\n     \"final_bandwidth\": 33,\n     \"alpha\": 10}),\n        \n    # round 3\n    ({\"primary_bandwidths\": np.arange(23, 48),\n     \"middle_bandwidth\": 46,\n     \"final_bandwidth\": 14,\n     \"alpha\": 8,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(103, 119),\n     \"middle_bandwidth\": 52,\n     \"final_bandwidth\": 31,\n     \"alpha\": 10}),\n    \n    ({\"primary_bandwidths\": np.arange(29, 41),\n     \"middle_bandwidth\": 54,\n     \"final_bandwidth\": 18,\n     \"alpha\": 5,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(111, 116),\n     \"middle_bandwidth\": 88,\n     \"final_bandwidth\": 37,\n     \"alpha\": 10}),\n    \n    ({\"primary_bandwidths\": np.arange(29, 43),\n     \"middle_bandwidth\": 50,\n     \"final_bandwidth\": 14,\n     \"alpha\": 7,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(112, 115),\n     \"middle_bandwidth\": 70,\n     \"final_bandwidth\": 37,\n     \"alpha\": 10}),\n    \n    # round 4\n    ({\"primary_bandwidths\": np.arange(16, 30),\n     \"middle_bandwidth\": 39,\n     \"final_bandwidth\": 29,\n     \"alpha\": 5,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(107, 116),\n     \"middle_bandwidth\": 156,\n     \"final_bandwidth\": 36,\n     \"alpha\": 0}),\n    \n    ({\"primary_bandwidths\": np.arange(15, 33),\n     \"middle_bandwidth\": 43,\n     \"final_bandwidth\": 25,\n     \"alpha\": 6,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(110, 114),\n     \"middle_bandwidth\": 154,\n     \"final_bandwidth\": 39,\n     \"alpha\": 0}),\n    \n    ({\"primary_bandwidths\": np.arange(16, 30),\n     \"middle_bandwidth\": 39,\n     \"final_bandwidth\": 29,\n     \"alpha\": 5,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(106, 118),\n     \"middle_bandwidth\": 155,\n     \"final_bandwidth\": 31,\n     \"alpha\": 0}),\n        \n    # round 5\n    ({\"primary_bandwidths\": np.arange(16, 18),\n     \"middle_bandwidth\": 31,\n     \"final_bandwidth\": 38,\n     \"alpha\": 9,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(112, 114),\n     \"middle_bandwidth\": 62,\n     \"final_bandwidth\": 36,\n     \"alpha\": 9}),\n        \n    ({\"primary_bandwidths\": np.arange(16, 18),\n     \"middle_bandwidth\": 31,\n     \"final_bandwidth\": 40,\n     \"alpha\": 4,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(111, 113),\n     \"middle_bandwidth\": 65,\n     \"final_bandwidth\": 37,\n     \"alpha\": 10}),\n    \n    ({\"primary_bandwidths\": np.arange(16, 18),\n     \"middle_bandwidth\": 32,\n     \"final_bandwidth\": 41,\n     \"alpha\": 8,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(109, 114),\n     \"middle_bandwidth\": 38,\n     \"final_bandwidth\": 39,\n     \"alpha\": 9}),\n    \n    # round 6      \n    ({\"primary_bandwidths\": np.arange(37, 39),\n     \"middle_bandwidth\": 28,\n     \"final_bandwidth\": 18,\n     \"alpha\": 2,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(108, 119),\n     \"middle_bandwidth\": 110,\n     \"final_bandwidth\": 34,\n     \"alpha\": 0}),\n        \n    ({\"primary_bandwidths\": np.arange(26, 40),\n     \"middle_bandwidth\": 34,\n     \"final_bandwidth\": 16,\n     \"alpha\": 2,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(108, 119),\n     \"middle_bandwidth\": 92,\n     \"final_bandwidth\": 35,\n     \"alpha\": 0}),\n    \n    ({\"primary_bandwidths\": np.arange(28, 40),\n     \"middle_bandwidth\": 44,\n     \"final_bandwidth\": 16,\n     \"alpha\": 0,\n      \"drop_last_n\": 1},\n     {\"primary_bandwidths\": np.arange(108, 119),\n     \"middle_bandwidth\": 85,\n     \"final_bandwidth\": 34,\n     \"alpha\": 2}),\n    \n    # round 7    \n    ({\"primary_bandwidths\": np.arange(41, 45),\n     \"middle_bandwidth\": 44,\n     \"final_bandwidth\": 15,\n     \"alpha\": 10,\n      \"drop_last_n\": 0},\n     {\"primary_bandwidths\": np.arange(112, 119),\n     \"middle_bandwidth\": 28,\n     \"final_bandwidth\": 33,\n     \"alpha\": 1}),\n    \n    ({\"primary_bandwidths\": np.arange(41, 45),\n     \"middle_bandwidth\": 48,\n     \"final_bandwidth\": 17,\n     \"alpha\": 9,\n      \"drop_last_n\": 0},\n     {\"primary_bandwidths\": np.arange(112, 119),\n     \"middle_bandwidth\": 124,\n     \"final_bandwidth\": 33,\n     \"alpha\": 3}),\n    \n    ({\"primary_bandwidths\": np.arange(17, 47),\n     \"middle_bandwidth\": 47,\n     \"final_bandwidth\": 16,\n     \"alpha\": 8,\n      \"drop_last_n\": 0},\n     {\"primary_bandwidths\": np.arange(112, 117),\n     \"middle_bandwidth\": 106,\n     \"final_bandwidth\": 36,\n     \"alpha\": 2})\n]"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["## Trend Correction auf store_id level ##\nfrom trend import apply_robust_trend_correction\nforecast_trend = apply_robust_trend_correction(data, forecast, level=3, kwargs_list=kwargs_list)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["data.groupby([\"store_id\",\"ds\"])[\"y\"].agg(\"sum\")"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["## Forecast mit trend correction (insgesamt) ##\nforecast_trend_agg = forecast_trend.groupby([\"ds\"])[\"y_pred\"].sum().reset_index()\nplt.figure(figsize=(20,7))\nplt.plot_date(data_agg[data_agg[\"ds\"]>= \"2015-01-01\"].ds, data_agg[data_agg[\"ds\"]>= \"2015-01-01\"].y, \"o-\", label=\"historic\",color = \"lightblue\")\nplt.plot_date(data_act.date, data_act.q, \"o-\", label=\"real\",color = \"blue\")\nplt.plot_date(forecast_agg.ds, forecast_agg.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\nplt.plot_date(forecast_trend_agg.ds, forecast_trend_agg.y_pred, \"o-\", label=\"trend corrected forecast\", color = \"green\")\nplt.grid()\nplt.legend(loc=\"best\")\nplt.show()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["## Forecast ohne trend correction und hierarchical reconciliation aggregiert (auf store_id) ##\nforecast_trend_agg_store = forecast_trend.groupby([\"ds\", \"store_id\"])[\"y_pred\"].sum().reset_index()\ndata_agg_store = data.groupby([\"ds\", \"store_id\"])[\"y\"].sum().reset_index()\ndata_act_store = actual_data.groupby([\"date\", \"store_id\"])[\"q\"].sum().reset_index()\nforecast_agg_store = forecast.groupby([\"ds\", \"store_id\"])[\"y_pred\"].sum().reset_index()\n\n\n\nfor store_id in range(1,10):\n  data_agg_store_i = data_agg_store[(data_agg_store[\"store_id\"] == store_id) & (data_agg_store[\"ds\"] >= \"2016-02-01\")]\n  forecast_agg_store_i = forecast_agg_store[forecast_agg_store[\"store_id\"] == store_id]\n  forecast_trend_agg_store_i = forecast_trend_agg_store[forecast_trend_agg_store[\"store_id\"] == store_id]\n  data_act_store_i = data_act_store[data_act_store[\"store_id\"] == store_id]\n\n  plt.figure(figsize=(25,8))\n  plt.plot_date(data_agg_store_i.ds, data_agg_store_i.y, \"o-\", label=\"historic\", color = \"lightblue\")\n  plt.plot_date(forecast_agg_store_i.ds, forecast_agg_store_i.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n  plt.plot_date(data_act_store_i.date, data_act_store_i.q, \"o-\", label=\"real\", color = \"navy\")\n  plt.plot_date(forecast_trend_agg_store_i.ds, forecast_trend_agg_store_i.y_pred, \"o-\", label=\"trend corrected forecast\", color = \"green\")\n  plt.grid()\n  plt.legend(loc=\"best\")\n  plt.show()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# Forecast mit trend_correction auf store_id und mit hierarchical reconciliation auf item_id #\n# from tqdm import tqdm\n\n## Einlesen und \"dekodieren\" der Hierarchie ##\nhierarchy_dict = {\"root\":hierarchy_raw.store_id.unique()}\n\nfor store_id in hierarchy_raw.store_id.unique():\n    hierarchy_dict[store_id] = hierarchy_raw.query(\"store_id == @store_id\").id.unique()\n\nhts = HTSDistributor(hierarchy_dict)\nforecast_trend_hts = forecast_trend.copy()\nforecast_trend_hts[\"store_id\"] = encoders[\"store\"].inverse_transform(forecast_trend_hts.store_id)\n\n## Wie viel wird pro Tag in allen stores verkauft ##\nforecast_level1_hts = forecast_trend_hts.groupby(\"ds\")[\"y_pred\"].sum().reset_index().set_index(\"ds\").rename({\"y_pred\":\"root\"}, axis=1)\n## Wie viel wird pro Tag pro Store verkauft ##\nforecast_trend_hts2 = forecast_trend_hts.pivot(index=\"ds\", columns=\"store_id\", values=\"y_pred\")\n## Wie viel wird pro Tag von jedem Item evrkauft ##\nforecast_hts = forecast.merge(hierarchy.loc[:, [\"id\",\"item_id\",\"store_id\"]], how=\"left\")\nforecast_level12_hts = forecast_hts.pivot(index=\"ds\", columns=\"id\", values=\"y_pred\")\n## Zusammeführen (eigtl mergen auf ds) ##\nforecast_concat = pd.concat([forecast_level1_hts, forecast_trend_hts2, forecast_level12_hts], axis=1)\n\n## Berechnen der Anteile der Spalten auf Bottomebene (Items / Store_ids) an der Top Ebenen Spalte ##\nfcst_reconc = hts.compute_forecast_proportions(forecast_concat)\nfcst_reconc.set_index(forecast_concat.index, inplace=True)\nfcst_reconc = fcst_reconc.loc[:, hts.bottom_nodes].transpose()\n\npredict_start = pd.to_datetime(\"2016-04-24\")\npredict_end = pd.to_datetime(\"2016-06-19\")\n\nforecast_reconc = (\n    fcst_reconc\n    .reset_index()\n    .rename({\"index\":\"id\"}, axis=1)\n    .melt(id_vars=\"id\", \n          value_vars=[predict_start+pd.DateOffset(days=i) for i in range(29)],\n          value_name=\"y_pred\"))\nforecast_reconc[\"id_encoded\"]= encoders[\"id\"].transform(forecast_reconc.id)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["for i in forecast_test[\"item_id\"].sample(15): \n  data_item = data[data[\"ts_id\"] == i].query(\"ds>='2016-02-01'\")\n  data_act_item = actual_data[actual_data[\"ts_id\"] == i]\n  forecast_item = forecast_test[forecast_test[\"ts_id\"] == i]\n  forecast_reconc_item = forecast_reconc[forecast_reconc[\"id_encoded\"] == i]\n  \n  plt.figure(figsize=(20,7))\n  plt.plot_date(data_item.ds, data_item.y, \"o-\", label=\"historic\", color = \"lightblue\")\n  plt.plot_date(data_act_item.date, data_act_item.q, \"o-\", label=\"real\", color = \"navy\")\n  plt.plot_date(forecast_item.ds, forecast_item.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n  plt.plot_date(forecast_reconc_item.ds, forecast_reconc_item.y_pred, \"o-\", label=\"reconc forecast\", color = \"green\")\n  plt.grid()\n  plt.legend(loc=\"best\")\n  plt.show()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["from typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            print(\"LV-weight\", lv_weight)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n            print(\"LV-weight Sum\", lv_weight.sum())\n\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        #print(\"Valid_y:\", valid_y)\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        print(\"Scale:\", scale)\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n\n        return np.mean(all_scores)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["weight"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["train_df"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["day_to_week = evaluator.calendar.set_index('d')['wm_yr_wk'].to_dict()\nweight_df = evaluator.train_df[['item_id', 'store_id'] + evaluator.weight_columns].set_index(['item_id', 'store_id'])\nweight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\nweight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["weight_df"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["weight_df = weight_df.merge(evaluator.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\nweight_df['value'] = weight_df['value'] * weight_df['sell_price']"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["weight_df"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\nweight_df"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["weight_df = weight_df.loc[zip(evaluator.train_df.item_id, evaluator.train_df.store_id), :].reset_index(drop=True)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["weight_df"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["weight_df = pd.concat([evaluator.train_df[evaluator.id_columns], weight_df], axis=1, sort=False)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["weight_df"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["weight_df"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["weight_df.groupby(\"store_id\")[weight_columns].sum().sum(axis = 1)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["getattr(evaluator, f'lv{3}_weight')"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["train_y = train_df.groupby(\"store_id\")[train_df.loc[:, train_df.columns.str.startswith('d_')].columns.tolist()].sum()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["for i, group_id in enumerate(evaluator.group_ids):\n  print(group_id, i)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":[" valid_y = getattr(evaluator, f'lv{3}_valid_df')"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["getattr(evaluator, f'lv{3}_scale')"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["valid_y"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["((valid_y - valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum())**2).mean(axis = 1)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["test = ((valid_y - valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum())**2).reset_index(drop = True)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["test.iloc[0].mean()"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["((valid_y - valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum())**2)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["lv_scores = evaluator.rmsse(valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum(), 3)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["lv_scores"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["lv_scores"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["valid_preds = pd.concat([evaluator.valid_df[evaluator.id_columns], forecast_right_format.reset_index(drop = True)], axis=1, sort=False)\n"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["valid_preds"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["evaluator = WRMSSEEvaluator(train_df =  new, valid_df = new2, calendar =  calendar, prices = sell_prices)\nevaluator.score(forecast_right_format.reset_index(drop = True))"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["evaluator.rmsse(new2)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["## Prepare data to calculate WRMSSE  ##\nvalid_df = sales_train.loc[:,\"d_1913\": \"d_1941\"]\ntrain_df = sales_train.drop(sales_train.loc[:,\"d_1913\": \"d_1941\"], axis = 1)\ntrain_df = train_df.drop({\"ts_id\"}, axis = 1)\n\n# [Formatieren für WRMSSE Berechnung]\nhelp_train = train_df[[\"id\", \"store_id\", \"item_id\"]]\ncalendar_merge = calendar[[\"date\", \"d\"]].rename({\"date\": \"ds\"}, axis = 1)\ncalendar_merge[\"ds\"] = pd.to_datetime(calendar_merge[\"ds\"])"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["# Calculate WRMSSE for forecast without trend correction and reconciliation ##\nforecast_id = help_train.reset_index().merge(forecast, on = [\"store_id\", \"item_id\"]).set_index('index')\nforecast_help = forecast_id.reset_index().merge(calendar_merge, on =\"ds\").set_index(\"index\")\nforecast_right_format = forecast_help[[\"d\", \"item_id\",\"store_id\",\"id\", \"y_pred\"]].pivot(columns =  \"d\", values = \"y_pred\")\nnew = train_df.reset_index(drop = True)\nnew2 = valid_df.reset_index(drop = True)"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["#valid_df: new2, calendar: pd.DataFrame, prices: pd.DataFrame):\ntrain_y = new.loc[:, new.columns.str.startswith('d_')]\ntrain_target_columns = train_y.columns.tolist()\nweight_columns = train_y.iloc[:, -28:].columns.tolist()"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["train_df['all_id'] = 0  # for lv1 aggregation\n\nid_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\nvalid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\nif not all([c in valid_df.columns for c in id_columns]):\n    valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["day_to_week = calendar.set_index('d')['wm_yr_wk'].to_dict()\nweight_df = train_df[['item_id', 'store_id'] + weight_columns].set_index(['item_id', 'store_id'])\nweight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\nweight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["evaluator = WRMSSEEvaluator(train_df =  new, valid_df = new2, calendar =  calendar, prices = sell_prices)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["evaluator = WRMSSEEvaluator(train_df =  new, valid_df = new2, calendar =  calendar, prices = sell_prices)\nevaluator.get_weight_df()\nevaluator.score(forecast_right_format.reset_index(drop = True))"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["## Calculate WRMSSE for forecast with trend correction and reconciliation ##\nforecast_reconc_wmrsse = train_df[[\"id\", \"store_id\", \"item_id\"]].reset_index().merge(forecast_reconc, on = \"id\").set_index(\"index\")\nforecast_reconc_wmrsse = forecast_reconc_wmrsse[[\"ds\",\"item_id\",\"store_id\",\"id\", \"y_pred\"]].reset_index().merge(calendar_merge, on = \"ds\").set_index(\"index\")\n\n# Pivot table #\nforecast_reconc_wmrsse = forecast_reconc_wmrsse[[\"d\", \"item_id\",\"store_id\",\"id\", \"y_pred\"]].pivot(columns =  \"d\", values = \"y_pred\")\nevaluator.score(forecast_reconc_wmrsse.reset_index(drop = True))\n"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["train_df[[\"id\", \"store_id\", \"item_id\"]]\n"],"metadata":{},"outputs":[],"execution_count":91}],"metadata":{"name":"m5_dataprep","notebookId":430997492991271},"nbformat":4,"nbformat_minor":0}