{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc\n","import os\n","# import pickle\n","import numpy as np \n","import pandas as pd \n","import category_encoders as ce\n","from category_encoders.ordinal import OrdinalEncoder\n","# import os\n","import copy\n","import time\n","# import numba\n","import lightgbm as lgb\n","#import click\n","#import tsforest \n","#from tsforest.forecast import LightGBMForecaster\n","from tsforest.utils import make_time_range\n","from scipy.stats import trim_mean\n","# import plotly.graph_objects as go \n","import matplotlib.pyplot as plt\n","#from tsforest.forecast import LightGBMForecaster\n","#from mahts import HTSDistributor\n","#import bayes_opt\n","\n","# COMMAND ----------\n","\n","import sys\n","#sys.path.append('/dbfs/Chronos/data/m5/')\n","\n","# COMMAND ----------\n","\n","#os.listdir(\"/dbfs/Chronos/data/m5/\")\n","\n","# COMMAND ----------"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","# import pickle\n","import numpy as np \n","import pandas as pd \n","import category_encoders as ce\n","from category_encoders.ordinal import OrdinalEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","## Read data ##\n","#os.listdir(\"/dbfs/Chronos/data/m5\")\n","#os.chdir(\"input/\")\n","calendar = pd.read_csv('input/calendar.csv')\n","sell_prices = pd.read_csv('input/sell_prices.csv')\n","sales_train = pd.read_csv('input/sales_train_evaluation.csv')\n","sales_train[\"id\"] = sales_train.id.map(lambda x: x.replace(\"_evaluation\", \"\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# COMMAND ----------\n","\n","sales_train[\"item_no\"] = sales_train[\"item_id\"].apply(lambda x: int(x[-3:]))\n","sales_train = sales_train[sales_train[\"item_no\"].isin(range(1, 600,2))]\n","sales_train = sales_train.drop(\"item_no\", axis = 1)\n","\n","# COMMAND ----------\n","\n","# ## Create Subset ##\n","# product = sales_train[['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n","# product[\"item_no\"] = product[\"item_id\"].apply(lambda x: x[-3:])\n","# product[\"item_no\"] = product[\"item_no\"].apply(lambda x: int(x)) ## Insgesamt 827 unique item_no\n","# product = product[product.cat_id.isin(['HOBBIES','FOODS'])]\n","\n","# product_subset = product[product[\"item_no\"] < 80] \n","# product_subset.drop(\"item_no\",axis=1,inplace=True)\n","# keys = list(product_subset.columns.values)\n","# i1 = sales_train.set_index(keys).index\n","# i2 = product_subset.set_index(keys).index\n","# sales_train = sales_train[i1.isin(i2)]\n","\n","# COMMAND ----------"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Hierarchical Encoding ##\n","hierarchy_raw = (sales_train.loc[:, [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]\n","             .drop_duplicates())\n","encoders = dict()\n","hierarchy = hierarchy_raw.copy()\n","\n","id_encoder = OrdinalEncoder()\n","id_encoder.fit(hierarchy.loc[:, [\"id\"]])\n","hierarchy[\"ts_id\"]  = id_encoder.transform(hierarchy.loc[:, [\"id\"]])\n","encoders[\"id\"] = id_encoder\n","\n","item_encoder = OrdinalEncoder()\n","item_encoder.fit(hierarchy.loc[:, [\"item_id\"]])\n","hierarchy.loc[:, \"item_id\"]  = item_encoder.transform(hierarchy.loc[:, [\"item_id\"]])\n","encoders[\"item\"] = item_encoder\n","\n","dept_encoder = OrdinalEncoder()\n","dept_encoder.fit(hierarchy.loc[:, [\"dept_id\"]])\n","hierarchy.loc[:, \"dept_id\"]  = dept_encoder.transform(hierarchy.loc[:, [\"dept_id\"]])\n","encoders[\"dept\"] = dept_encoder\n","\n","cat_encoder = OrdinalEncoder()\n","cat_encoder.fit(hierarchy.loc[:, [\"cat_id\"]])\n","hierarchy.loc[:, \"cat_id\"]   = cat_encoder.transform(hierarchy.loc[:, [\"cat_id\"]])\n","encoders[\"cat\"] = cat_encoder\n","\n","store_encoder = OrdinalEncoder()\n","store_encoder.fit(hierarchy.loc[:, [\"store_id\"]])\n","hierarchy.loc[:, \"store_id\"] = store_encoder.transform(hierarchy.loc[:, [\"store_id\"]])\n","encoders[\"store\"] = store_encoder\n","\n","state_encoder = OrdinalEncoder()\n","state_encoder.fit(hierarchy.loc[:, [\"state_id\"]])\n","hierarchy.loc[:, \"state_id\"] = state_encoder.transform(hierarchy.loc[:, [\"state_id\"]])\n","encoders[\"state\"] = state_encoder\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Encode Calendar Events ##\n","event_name_1_encoder = OrdinalEncoder()\n","event_name_1_encoder.fit(calendar.loc[:, [\"event_name_1\"]])\n","calendar.loc[:, \"event_name_1\"] = event_name_1_encoder.transform(calendar.loc[:, [\"event_name_1\"]])\n","\n","event_type_1_encoder = OrdinalEncoder()\n","event_type_1_encoder.fit(calendar.loc[:, [\"event_type_1\"]])\n","calendar.loc[:, \"event_type_1\"] = event_type_1_encoder.transform(calendar.loc[:, [\"event_type_1\"]])\n","\n","event_name_2_encoder = OrdinalEncoder()\n","event_name_2_encoder.fit(calendar.loc[:, [\"event_name_2\"]])\n","calendar.loc[:, \"event_name_2\"] = event_name_2_encoder.transform(calendar.loc[:, [\"event_name_2\"]])\n","\n","event_type_2_encoder = OrdinalEncoder()\n","event_type_2_encoder.fit(calendar.loc[:, [\"event_type_2\"]])\n","calendar.loc[:, \"event_type_2\"] = event_type_2_encoder.transform(calendar.loc[:, [\"event_type_2\"]])\n","\n","# COMMAND ----------\n","\n","## Enocde categorical features ##\n","sales_train[\"ts_id\"] = id_encoder.transform(sales_train.loc[:, [\"id\"]])\n","sales_train.loc[:, \"item_id\"]  = item_encoder.transform(sales_train.loc[:, [\"item_id\"]])\n","sales_train.loc[:, \"dept_id\"]  = dept_encoder.transform(sales_train.loc[:, [\"dept_id\"]])\n","sales_train.loc[:, \"cat_id\"]   = cat_encoder.transform(sales_train.loc[:, [\"cat_id\"]])\n","sales_train.loc[:, \"store_id\"] = store_encoder.transform(sales_train.loc[:, [\"store_id\"]])\n","sales_train.loc[:, \"state_id\"] = state_encoder.transform(sales_train.loc[:, [\"state_id\"]])\n","\n","## Encode features in sell_prices ##\n","sell_prices.loc[:, \"store_id\"] = store_encoder.transform(sell_prices.loc[:, [\"store_id\"]])\n","sell_prices.loc[:, \"item_id\"]  = item_encoder.transform(sell_prices.loc[:, [\"item_id\"]]) \n","\n","# COMMAND ----------\n","\n","actual_data = pd.melt(sales_train, \n","               id_vars=[\"ts_id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"],\n","               value_vars=[f\"d_{i}\" for i in range(1913,1942)],\n","               var_name=\"d\",\n","               value_name=\"q\")\n","actual_data = actual_data.merge(calendar[[\"d\",\"date\"]], on = \"d\").drop(\"d\", axis = 1)\n","actual_data[\"date\"] = pd.to_datetime(actual_data[\"date\"])\n","\n","# COMMAND ----------\n","\n","## Melt and Merge Datasets ##\n","data = pd.melt(sales_train, \n","               id_vars=[\"ts_id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\"],\n","               value_vars=[f\"d_{i}\" for i in range(1,1913)],\n","               var_name=\"d\",\n","               value_name=\"q\")\n","\n","calendar_columns = [\"date\", \"wm_yr_wk\", \"d\", \"snap_CA\", \"snap_TX\", \"snap_WI\",\n","                    \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n","\n","data = pd.merge(data, \n","                calendar.loc[:, calendar_columns],\n","                how=\"left\",\n","                on=\"d\")\n","\n","data = pd.merge(data, sell_prices,\n","                on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],\n","                how=\"left\")\n","\n","data.sort_values([\"item_id\",\"store_id\",\"date\"], inplace=True, ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# state_encoder.mapping[0][\"mapping\"]\n","\n","# COMMAND ----------\n","\n","## Encode snap-features ##\n","data[\"snap\"] = 0\n","\n","idx_snap_ca = data.query(\"state_id==1 & snap_CA==1\").index\n","data.loc[idx_snap_ca, \"snap\"] = 1\n","\n","idx_snap_tx = data.query(\"state_id==2 & snap_TX==1\").index\n","data.loc[idx_snap_tx, \"snap\"] = 2\n","\n","idx_snap_wi = data.query(\"state_id==3 & snap_WI==1\").index\n","data.loc[idx_snap_wi, \"snap\"] = 3\n","\n","data.drop([\"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1, inplace=True)\n","\n","# COMMAND ----------\n","\n","\n","def reduce_mem_usage(df, verbose=False):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2    \n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                #    df[col] = df[col].astype(np.float16)\n","                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)    \n","    end_mem = df.memory_usage().sum() / 1024**2\n","    if verbose: \n","        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","    return df\n","#data = reduce_mem_usage(data)\n","\n","# COMMAND ----------\n","\n","def remove_starting_zeros(dataframe):\n","    idxmin = dataframe.query(\"q > 0\").index.min()\n","    return dataframe.loc[idxmin:, :]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = (data\n","        .groupby([\"item_id\",\"store_id\"])\n","        .apply(remove_starting_zeros)\n","        .reset_index(drop=True)\n","       )\n","\n","# COMMAND ----------\n","\n","def find_out_of_stock(df):\n","    df = df.copy()\n","\n","    df[\"no_stock_days\"] = 0\n","    zero_mask = (df.q == 0)\n","    transition_mask = (zero_mask != zero_mask.shift(1)) #TRUE, falls es von 0 zu !0 (oder umgekehrt) übergeht\n","    zero_sequences = transition_mask.cumsum()[zero_mask] ## Aufsummieren aller TRUE (1) \n","    zero_seqs_count = zero_sequences.map(zero_sequences.value_counts()).to_frame() ## Zähle wie häufig die jeweiligen kumulierten Werte auftreten\n","    df.loc[zero_seqs_count.index, \"no_stock_days\"] = zero_seqs_count.q.values\n","    return df\n","\n","## no_stock_data gibt an wie lange die Periode ist, für die das jeweilige Item im jeweiligen Store nicht verkauft wird. \n","## Ist q für ein Item in eime Store für 4 Tage lang 0 (= es wird vier Tage lang nicht verkauft), so ist no_stock_data für \n","## alle vier Zeitpunkte gleich 4 (siehe z.B. data.tail(20)) \n","data = data.groupby([\"item_id\",\"store_id\"]).apply(find_out_of_stock)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# COMMAND ----------\n","\n","data.reset_index(drop=True, inplace=True)\n","data.drop([\"d\", \"wm_yr_wk\"], axis=1, inplace=True)\n","data.rename({\"date\":\"ds\"}, axis=1, inplace=True)\n","data = data.rename({\"q\":\"y\"}, axis=1)\n","data[\"ds\"] = pd.to_datetime(data[\"ds\"])\n","\n","\n","# COMMAND ----------\n","\n","## Create evaluation dataframe ##\n","calendar_columns = [\"date\", \"wm_yr_wk\", \"snap_CA\", \"snap_TX\", \"snap_WI\",\n","                    \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n","calendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n","\n","eval_dataframe = (pd.concat([make_time_range(\"2016-04-24\", \"2016-05-22\", \"D\").assign(**row)\n","                             for _,row in hierarchy.iterrows()], ignore_index=True)\n","                  .merge(calendar.loc[:, calendar_columns],\n","                         how=\"left\", left_on=\"ds\", right_on=\"date\")\n","                  .merge(sell_prices, how=\"left\")\n","                  .drop([\"id\",\"date\",\"wm_yr_wk\"], axis=1)\n","                 )\n","\n","eval_dataframe[\"snap\"] = 0\n","\n","idx_snap_ca = eval_dataframe.query(\"state_id==1 & snap_CA==1\").index\n","eval_dataframe.loc[idx_snap_ca, \"snap\"] = 1\n","\n","idx_snap_tx = eval_dataframe.query(\"state_id==2 & snap_TX==1\").index\n","eval_dataframe.loc[idx_snap_tx, \"snap\"] = 2\n","\n","idx_snap_wi = eval_dataframe.query(\"state_id==3 & snap_WI==1\").index\n","eval_dataframe.loc[idx_snap_wi, \"snap\"] = 3\n","\n","eval_dataframe.drop([\"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1, inplace=True)\n","\n","eval_dataframe[\"no_stock_days\"] = None\n","eval_dataframe[\"ds\"] = pd.to_datetime(eval_dataframe[\"ds\"])\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# COMMAND ----------\n","\n","## Set model parameter ##\n","def compute_czeros(x):\n","  return np.sum(np.cumprod((x==0)[::-1]))/x.shape[0]\n","\n","def compute_sfreq(x):\n","  return np.sum(x!=0)/x.shape[0]\n","\n","approach=1\n","\n","model_params = {\n","  'objective':'tweedie',\n","  'tweedie_variance_power': 1.1,\n","  'metric':'None',\n","  'max_bin': 127,\n","  'bin_construct_sample_cnt':20000000,\n","  'num_leaves': 2**10-1,\n","  'min_data_in_leaf': 2**10-1,\n","  'learning_rate': 0.05,\n","  'feature_fraction':0.8,\n","  'bagging_fraction':0.8,\n","  'bagging_freq':1,\n","  'lambda_l2':0.1,\n","  'boost_from_average': False,\n","}\n","\n","time_features = [\n","  \"year\",\n","  \"month\",\n","  #\"year_week\",\n","  #\"year_day\",\n","  \"week_day\",\n","  \"month_progress\",\n","  #\"week_day_cos\",\n","  #\"week_day_sin\",\n","  #\"year_day_cos\",\n","  #\"year_day_sin\",\n","  \"year_week_cos\",\n","  \"year_week_sin\",\n","  #\"month_cos\",\n","  #\"month_sin\"\n","]\n","\n","exclude_features = [\n","  \"ts_id\",\n","  \"no_stock_days\",\n","  \"sales\",\n","]\n","\n","categorical_features = {\n","  \"store_id\": \"default\",  ## Wird automatisch mit \"default\" hinzugefügt, da in ts_uid_columns\n","  \"item_id\": \"default\", ## Wird automatisch mit \"default\" hinzugefügt, da in ts_uid_columns\n","  \"state_id\": \"default\",\n","  \"dept_id\": \"default\",\n","  \"cat_id\": \"default\",\n","  \"event_name_1\": \"default\",}\n","if approach == 1:\n","  categorical_features[\"item_id\"] = \"default\"\n","elif approach == 2:\n","  categorical_features[\"item_id\"] = (\"y\", ce.GLMMEncoder, None)\n","else:\n","  print(\"Invalid input.\")\n","\n","model_kwargs = {\n","  \"model_params\":model_params,\n","  \"time_features\":['week_day', 'month_progress', 'year_week_cos', 'year_week_sin'],\n","  \"window_functions\":{\n","    \"mean\":   (None, [1,7,28], [7,14,28]),\n","      \"std\":    (None, [1,7,28], [7,14,28]),\n","    \"kurt\":   (None, [1,7,28], [7,28]),\n","    \"czeros\": (compute_czeros, [1,], [7,14])\n","    },\n","  \"exclude_features\":exclude_features,\n","  \"categorical_features\":categorical_features,\n","  \"ts_uid_columns\":[\"item_id\",\"store_id\"],\n","  #\"ts_uid_columns\": [\"store_id\"]\n","}\n","\n","lagged_features_to_dropna = list()\n","if \"lags\" in model_kwargs.keys():\n","    lag_features = [f\"lag{lag}\" for lag in model_kwargs[\"lags\"]]\n","    lagged_features_to_dropna.extend(lag_features)\n","if \"window_functions\" in model_kwargs.keys():\n","    rw_features = list()\n","    for window_func,window_def in model_kwargs[\"window_functions\"].items():\n","        _,window_shifts,window_sizes = window_def\n","        if window_func in [\"mean\",\"median\",\"std\",\"min\",\"max\"]:\n","            rw_features.extend([f\"{window_func}{window_size}_shift{window_shift}\"\n","                                for window_size in window_sizes\n","                                for window_shift in window_shifts])\n","    lagged_features_to_dropna.extend(rw_features)\n","\n","# SEEDS = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, \n","#          43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n","# NUM_ITER_RANGE = (500,701)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datatable as dt\n","data = dt.Frame(data)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.to_jay(\"daten.jay\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# COMMAND ----------\n","\n","## Create modell with model parameters and set features ##\n","tic = time.time()\n","model_level12_base = LightGBMForecaster(**model_kwargs) ## Erzeugen eines Objekts LightGBMForecaster, das von ForecasterBase erbt\n","model_level12_base.prepare_features(train_data=data) ## Methode aus ForecasterBase\n","model_level12_base.train_features.dropna(subset=lagged_features_to_dropna, axis=0, inplace=True) ## Methode aus ForecasterBase\n","# model_level12_base.train_features = reduce_mem_usage(model_level12_base.train_features) ## Methode aus ForecasterBase\n","gc.collect()\n","tac = time.time()\n","print(f\"Elapsed time: {(tac-tic)/ 60.} [min]\")\n","\n","# COMMAND ----------\n","\n","## Fit the model ## \n","# test = list()\n","#for i,seed in enumerate(SEEDS[1]):   \n","#num_iterations = np.random.randint(*NUM_ITER_RANGE)\n","num_iterations = 600\n","seed = 3\n","#print(\"#\"*100)\n","# print(f\" model {i+1}/{len(SEEDS)} - seed: {seed} - num_iterations: {num_iterations} \".center(100, \"#\"))\n","#print(\"#\"*100)\n","\n","model_level12 = copy.deepcopy(model_level12_base)\n","model_params[\"seed\"] = seed\n","model_params[\"num_iterations\"] = num_iterations\n","model_level12.set_params(model_params)\n","\n","print(\"Fitting the model\")\n","tic = time.time()\n","model_level12.fit()\n","tac = time.time()\n","print(f\"Elapsed time: {(tac-tic)/60.} [min]\")\n","\n","# COMMAND ----------\n","\n","## Predict values ## \n","# thresh_value\n","predict_data = model_level12.train_features.query(\"no_stock_days >= 28\").loc[:, model_level12.input_features]\n","predictions = model_level12.model.model.predict(predict_data) ## Aufrufen der predict methode von lightgbm.train\n","print(predictions)\n","thresh_value = trim_mean(predictions, proportiontocut=0.05)\n","print(thresh_value)\n","def bias_corr_func(x, tv=thresh_value):\n","    x[x < tv] = 0\n","    return x\n","\n","print(\"Predicting\")\n","tic = time.time()\n","forecast = model_level12.predict(eval_dataframe, recursive=True, bias_corr_func=bias_corr_func) ## Erzeugen einer rekursiven Vorhersage \n","## Rekursiv beduetet hier, dass die lag-feature in Periode t abhängig von den in Periode t-1 berechneten lag-featuren ermittlet werden\n","print(forecast)\n","tac = time.time()\n","# test.append(forecast)\n","print(f\"Elapsed time: {(tac-tic)/60.} [min]\")\n","\n","\n","# COMMAND ----------\n","\n","forecast_test = forecast.merge(actual_data[[\"ts_id\",\"item_id\", \"store_id\", \"date\"]], left_on = [\"ds\",\"item_id\", \"store_id\"], right_on = [\"date\",\"item_id\", \"store_id\"])\n","\n","# COMMAND ----------\n","\n","## Feature importance ##\n","# item_id ist GLMM-Encoded und daher nicht mehr so relevant ##\n","lgb.plot_importance(model_level12.model.model, importance_type=\"split\", figsize=(15,10))\n","\n","\n","# COMMAND ----------\n","\n","## Forecast ohne trend correction und hierarchical reconciliation aggregiert (insgesamt) ##\n","data_agg = data.groupby([\"ds\"])[\"y\"].sum().reset_index()\n","data_act = actual_data.groupby([\"date\"])[\"q\"].sum().reset_index()\n","forecast_agg = forecast.groupby([\"ds\"])[\"y_pred\"].sum().reset_index()\n","\n","plt.figure(figsize=(20,7))\n","plt.plot_date(data_agg.ds, data_agg.y, \"o-\", label=\"historic\", color = \"lightblue\")\n","plt.plot_date(data_act.date, data_act.q, \"o-\", label=\"original\", color = \"navy\")\n","plt.plot_date(forecast_agg.ds, forecast_agg.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n","plt.grid()\n","plt.legend(loc=\"best\")\n","plt.show()\n","\n","# COMMAND ----------\n","\n","# Forecast ohne trend correction und hierarchical reconciliation aggregiert (auf store_id) ##\n","data_agg_store = data.groupby([\"ds\", \"store_id\"])[\"y\"].sum().reset_index()\n","data_act_store = actual_data.groupby([\"date\", \"store_id\"])[\"q\"].sum().reset_index()\n","forecast_agg_store = forecast.groupby([\"ds\", \"store_id\"])[\"y_pred\"].sum().reset_index()\n","\n","for store_id in range(1,10):\n","  data_agg_store_i = data_agg_store[data_agg_store[\"store_id\"] == store_id]\n","  data_act_store_i = data_act_store[data_act_store[\"store_id\"] == store_id]\n","  forecast_agg_store_i = forecast_agg_store[forecast_agg_store[\"store_id\"] == store_id]\n","\n","  plt.figure(figsize=(20,7))\n","  plt.plot_date(data_agg_store_i.ds, data_agg_store_i.y, \"o-\", label=\"original\", color = \"lightblue\")\n","  plt.plot_date(data_act_store_i.date, data_act_store_i.q, \"o-\", label=\"original\", color = \"navy\")\n","  plt.plot_date(forecast_agg_store_i.ds, forecast_agg_store_i.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n","  plt.grid()\n","  plt.legend(loc=\"best\")\n","  plt.show()\n","\n","# COMMAND ----------\n","\n","## Forecast ohne trend correction und hierarchical reconciliation für item_id = 4# \n","# data_item = data[data[\"item_id\"] == 4]\n","# data_act_item = actual_data[actual_data[\"item_id\"] == 4]\n","# forecast_item = forecast[forecast[\"item_id\"] == 4]\n","\n","for i in range(1,10): \n","  data_item = data[data[\"ts_id\"] == i]\n","  data_act_item = actual_data[actual_data[\"ts_id\"] == i]\n","  forecast_item = forecast_test[forecast_test[\"ts_id\"] == i]\n","  plt.figure(figsize=(20,7))\n","  plt.plot_date(data_item.ds, data_item.y, \"o-\", label=\"original\", color = \"lightblue\")\n","  plt.plot_date(data_act_item.date, data_act_item.q, \"o-\", label=\"original\", color = \"navy\")\n","  plt.plot_date(forecast_item.ds, forecast_item.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n","  plt.grid()\n","  plt.legend(loc=\"best\")\n","  plt.show()\n","\n","# COMMAND ----------\n","\n","## Set parameters for trend correction model ##\n","kwargs_list = [\n","    # round 0\n","    ({\"primary_bandwidths\": np.arange(41, 47),\n","     \"middle_bandwidth\": 33,\n","     \"final_bandwidth\": 15,\n","     \"alpha\": 4,\n","      \"drop_last_n\": 0},\n","     {\"primary_bandwidths\": np.arange(112, 119),\n","     \"middle_bandwidth\": 38,\n","     \"final_bandwidth\": 33,\n","     \"alpha\": 2}),\n","    \n","    ({\"primary_bandwidths\": np.arange(42, 46),\n","     \"middle_bandwidth\": 19,\n","     \"final_bandwidth\": 18,\n","     \"alpha\": 4,\n","      \"drop_last_n\": 0},\n","     {\"primary_bandwidths\": np.arange(112, 119),\n","     \"middle_bandwidth\": 30,\n","     \"final_bandwidth\": 33,\n","     \"alpha\": 1}),\n","    \n","    ({\"primary_bandwidths\": np.arange(42, 46),\n","     \"middle_bandwidth\": 35,\n","     \"final_bandwidth\": 15,\n","     \"alpha\": 4,\n","      \"drop_last_n\": 0},\n","     {\"primary_bandwidths\": np.arange(112, 119),\n","     \"middle_bandwidth\": 42,\n","     \"final_bandwidth\": 33,\n","     \"alpha\": 10}),\n","    \n","    # round 2\n","    ({\"primary_bandwidths\": np.arange(20, 55),\n","     \"middle_bandwidth\": 55,\n","     \"final_bandwidth\": 38,\n","     \"alpha\": 0,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(112, 119),\n","     \"middle_bandwidth\": 43,\n","     \"final_bandwidth\": 33,\n","     \"alpha\": 0}),\n","    \n","    ({\"primary_bandwidths\": np.arange(21, 54),\n","     \"middle_bandwidth\": 55,\n","     \"final_bandwidth\": 41,\n","     \"alpha\": 1,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(112, 119),\n","     \"middle_bandwidth\": 41,\n","     \"final_bandwidth\": 33,\n","     \"alpha\": 0}),\n","    \n","    ({\"primary_bandwidths\": np.arange(24, 56),\n","     \"middle_bandwidth\": 54,\n","     \"final_bandwidth\": 41,\n","     \"alpha\": 1,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(112, 119),\n","     \"middle_bandwidth\": 31,\n","     \"final_bandwidth\": 33,\n","     \"alpha\": 10}),\n","        \n","    # round 3\n","    ({\"primary_bandwidths\": np.arange(23, 48),\n","     \"middle_bandwidth\": 46,\n","     \"final_bandwidth\": 14,\n","     \"alpha\": 8,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(103, 119),\n","     \"middle_bandwidth\": 52,\n","     \"final_bandwidth\": 31,\n","     \"alpha\": 10}),\n","    \n","    ({\"primary_bandwidths\": np.arange(29, 41),\n","     \"middle_bandwidth\": 54,\n","     \"final_bandwidth\": 18,\n","     \"alpha\": 5,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(111, 116),\n","     \"middle_bandwidth\": 88,\n","     \"final_bandwidth\": 37,\n","     \"alpha\": 10}),\n","    \n","    ({\"primary_bandwidths\": np.arange(29, 43),\n","     \"middle_bandwidth\": 50,\n","     \"final_bandwidth\": 14,\n","     \"alpha\": 7,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(112, 115),\n","     \"middle_bandwidth\": 70,\n","     \"final_bandwidth\": 37,\n","     \"alpha\": 10}),\n","    \n","    # round 4\n","    ({\"primary_bandwidths\": np.arange(16, 30),\n","     \"middle_bandwidth\": 39,\n","     \"final_bandwidth\": 29,\n","     \"alpha\": 5,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(107, 116),\n","     \"middle_bandwidth\": 156,\n","     \"final_bandwidth\": 36,\n","     \"alpha\": 0}),\n","    \n","    ({\"primary_bandwidths\": np.arange(15, 33),\n","     \"middle_bandwidth\": 43,\n","     \"final_bandwidth\": 25,\n","     \"alpha\": 6,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(110, 114),\n","     \"middle_bandwidth\": 154,\n","     \"final_bandwidth\": 39,\n","     \"alpha\": 0}),\n","    \n","    ({\"primary_bandwidths\": np.arange(16, 30),\n","     \"middle_bandwidth\": 39,\n","     \"final_bandwidth\": 29,\n","     \"alpha\": 5,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(106, 118),\n","     \"middle_bandwidth\": 155,\n","     \"final_bandwidth\": 31,\n","     \"alpha\": 0}),\n","        \n","    # round 5\n","    ({\"primary_bandwidths\": np.arange(16, 18),\n","     \"middle_bandwidth\": 31,\n","     \"final_bandwidth\": 38,\n","     \"alpha\": 9,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(112, 114),\n","     \"middle_bandwidth\": 62,\n","     \"final_bandwidth\": 36,\n","     \"alpha\": 9}),\n","        \n","    ({\"primary_bandwidths\": np.arange(16, 18),\n","     \"middle_bandwidth\": 31,\n","     \"final_bandwidth\": 40,\n","     \"alpha\": 4,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(111, 113),\n","     \"middle_bandwidth\": 65,\n","     \"final_bandwidth\": 37,\n","     \"alpha\": 10}),\n","    \n","    ({\"primary_bandwidths\": np.arange(16, 18),\n","     \"middle_bandwidth\": 32,\n","     \"final_bandwidth\": 41,\n","     \"alpha\": 8,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(109, 114),\n","     \"middle_bandwidth\": 38,\n","     \"final_bandwidth\": 39,\n","     \"alpha\": 9}),\n","    \n","    # round 6      \n","    ({\"primary_bandwidths\": np.arange(37, 39),\n","     \"middle_bandwidth\": 28,\n","     \"final_bandwidth\": 18,\n","     \"alpha\": 2,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(108, 119),\n","     \"middle_bandwidth\": 110,\n","     \"final_bandwidth\": 34,\n","     \"alpha\": 0}),\n","        \n","    ({\"primary_bandwidths\": np.arange(26, 40),\n","     \"middle_bandwidth\": 34,\n","     \"final_bandwidth\": 16,\n","     \"alpha\": 2,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(108, 119),\n","     \"middle_bandwidth\": 92,\n","     \"final_bandwidth\": 35,\n","     \"alpha\": 0}),\n","    \n","    ({\"primary_bandwidths\": np.arange(28, 40),\n","     \"middle_bandwidth\": 44,\n","     \"final_bandwidth\": 16,\n","     \"alpha\": 0,\n","      \"drop_last_n\": 1},\n","     {\"primary_bandwidths\": np.arange(108, 119),\n","     \"middle_bandwidth\": 85,\n","     \"final_bandwidth\": 34,\n","     \"alpha\": 2}),\n","    \n","    # round 7    \n","    ({\"primary_bandwidths\": np.arange(41, 45),\n","     \"middle_bandwidth\": 44,\n","     \"final_bandwidth\": 15,\n","     \"alpha\": 10,\n","      \"drop_last_n\": 0},\n","     {\"primary_bandwidths\": np.arange(112, 119),\n","     \"middle_bandwidth\": 28,\n","     \"final_bandwidth\": 33,\n","     \"alpha\": 1}),\n","    \n","    ({\"primary_bandwidths\": np.arange(41, 45),\n","     \"middle_bandwidth\": 48,\n","     \"final_bandwidth\": 17,\n","     \"alpha\": 9,\n","      \"drop_last_n\": 0},\n","     {\"primary_bandwidths\": np.arange(112, 119),\n","     \"middle_bandwidth\": 124,\n","     \"final_bandwidth\": 33,\n","     \"alpha\": 3}),\n","    \n","    ({\"primary_bandwidths\": np.arange(17, 47),\n","     \"middle_bandwidth\": 47,\n","     \"final_bandwidth\": 16,\n","     \"alpha\": 8,\n","      \"drop_last_n\": 0},\n","     {\"primary_bandwidths\": np.arange(112, 117),\n","     \"middle_bandwidth\": 106,\n","     \"final_bandwidth\": 36,\n","     \"alpha\": 2})\n","]\n","\n","# COMMAND ----------\n","\n","\n","\n","# COMMAND ----------\n","\n","## Trend Correction auf store_id level ##\n","from trend import apply_robust_trend_correction\n","forecast_trend = apply_robust_trend_correction(data, forecast, level=3, kwargs_list=kwargs_list)\n","\n","# COMMAND ----------\n","\n","data.groupby([\"store_id\",\"ds\"])[\"y\"].agg(\"sum\")\n","\n","# COMMAND ----------\n","\n","## Forecast mit trend correction (insgesamt) ##\n","forecast_trend_agg = forecast_trend.groupby([\"ds\"])[\"y_pred\"].sum().reset_index()\n","plt.figure(figsize=(20,7))\n","plt.plot_date(data_agg[data_agg[\"ds\"]>= \"2015-01-01\"].ds, data_agg[data_agg[\"ds\"]>= \"2015-01-01\"].y, \"o-\", label=\"historic\",color = \"lightblue\")\n","plt.plot_date(data_act.date, data_act.q, \"o-\", label=\"real\",color = \"blue\")\n","plt.plot_date(forecast_agg.ds, forecast_agg.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n","plt.plot_date(forecast_trend_agg.ds, forecast_trend_agg.y_pred, \"o-\", label=\"trend corrected forecast\", color = \"green\")\n","plt.grid()\n","plt.legend(loc=\"best\")\n","plt.show()\n","\n","# COMMAND ----------\n","\n","## Forecast ohne trend correction und hierarchical reconciliation aggregiert (auf store_id) ##\n","forecast_trend_agg_store = forecast_trend.groupby([\"ds\", \"store_id\"])[\"y_pred\"].sum().reset_index()\n","data_agg_store = data.groupby([\"ds\", \"store_id\"])[\"y\"].sum().reset_index()\n","data_act_store = actual_data.groupby([\"date\", \"store_id\"])[\"q\"].sum().reset_index()\n","forecast_agg_store = forecast.groupby([\"ds\", \"store_id\"])[\"y_pred\"].sum().reset_index()\n","\n","\n","\n","for store_id in range(1,10):\n","  data_agg_store_i = data_agg_store[(data_agg_store[\"store_id\"] == store_id) & (data_agg_store[\"ds\"] >= \"2016-02-01\")]\n","  forecast_agg_store_i = forecast_agg_store[forecast_agg_store[\"store_id\"] == store_id]\n","  forecast_trend_agg_store_i = forecast_trend_agg_store[forecast_trend_agg_store[\"store_id\"] == store_id]\n","  data_act_store_i = data_act_store[data_act_store[\"store_id\"] == store_id]\n","\n","  plt.figure(figsize=(25,8))\n","  plt.plot_date(data_agg_store_i.ds, data_agg_store_i.y, \"o-\", label=\"historic\", color = \"lightblue\")\n","  plt.plot_date(forecast_agg_store_i.ds, forecast_agg_store_i.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n","  plt.plot_date(data_act_store_i.date, data_act_store_i.q, \"o-\", label=\"real\", color = \"navy\")\n","  plt.plot_date(forecast_trend_agg_store_i.ds, forecast_trend_agg_store_i.y_pred, \"o-\", label=\"trend corrected forecast\", color = \"green\")\n","  plt.grid()\n","  plt.legend(loc=\"best\")\n","  plt.show()\n","\n","# COMMAND ----------\n","\n","# Forecast mit trend_correction auf store_id und mit hierarchical reconciliation auf item_id #\n","# from tqdm import tqdm\n","\n","## Einlesen und \"dekodieren\" der Hierarchie ##\n","hierarchy_dict = {\"root\":hierarchy_raw.store_id.unique()}\n","\n","for store_id in hierarchy_raw.store_id.unique():\n","    hierarchy_dict[store_id] = hierarchy_raw.query(\"store_id == @store_id\").id.unique()\n","\n","hts = HTSDistributor(hierarchy_dict)\n","forecast_trend_hts = forecast_trend.copy()\n","forecast_trend_hts[\"store_id\"] = encoders[\"store\"].inverse_transform(forecast_trend_hts.store_id)\n","\n","## Wie viel wird pro Tag in allen stores verkauft ##\n","forecast_level1_hts = forecast_trend_hts.groupby(\"ds\")[\"y_pred\"].sum().reset_index().set_index(\"ds\").rename({\"y_pred\":\"root\"}, axis=1)\n","## Wie viel wird pro Tag pro Store verkauft ##\n","forecast_trend_hts2 = forecast_trend_hts.pivot(index=\"ds\", columns=\"store_id\", values=\"y_pred\")\n","## Wie viel wird pro Tag von jedem Item evrkauft ##\n","forecast_hts = forecast.merge(hierarchy.loc[:, [\"id\",\"item_id\",\"store_id\"]], how=\"left\")\n","forecast_level12_hts = forecast_hts.pivot(index=\"ds\", columns=\"id\", values=\"y_pred\")\n","## Zusammeführen (eigtl mergen auf ds) ##\n","forecast_concat = pd.concat([forecast_level1_hts, forecast_trend_hts2, forecast_level12_hts], axis=1)\n","\n","## Berechnen der Anteile der Spalten auf Bottomebene (Items / Store_ids) an der Top Ebenen Spalte ##\n","fcst_reconc = hts.compute_forecast_proportions(forecast_concat)\n","fcst_reconc.set_index(forecast_concat.index, inplace=True)\n","fcst_reconc = fcst_reconc.loc[:, hts.bottom_nodes].transpose()\n","\n","predict_start = pd.to_datetime(\"2016-04-24\")\n","predict_end = pd.to_datetime(\"2016-06-19\")\n","\n","forecast_reconc = (\n","    fcst_reconc\n","    .reset_index()\n","    .rename({\"index\":\"id\"}, axis=1)\n","    .melt(id_vars=\"id\", \n","          value_vars=[predict_start+pd.DateOffset(days=i) for i in range(29)],\n","          value_name=\"y_pred\"))\n","forecast_reconc[\"id_encoded\"]= encoders[\"id\"].transform(forecast_reconc.id)\n","\n","# COMMAND ----------\n","\n","for i in forecast_test[\"item_id\"].sample(15): \n","  data_item = data[data[\"ts_id\"] == i].query(\"ds>='2016-02-01'\")\n","  data_act_item = actual_data[actual_data[\"ts_id\"] == i]\n","  forecast_item = forecast_test[forecast_test[\"ts_id\"] == i]\n","  forecast_reconc_item = forecast_reconc[forecast_reconc[\"id_encoded\"] == i]\n","  \n","  plt.figure(figsize=(20,7))\n","  plt.plot_date(data_item.ds, data_item.y, \"o-\", label=\"historic\", color = \"lightblue\")\n","  plt.plot_date(data_act_item.date, data_act_item.q, \"o-\", label=\"real\", color = \"navy\")\n","  plt.plot_date(forecast_item.ds, forecast_item.y_pred, \"o-\", label=\"einfacher forecast\", color = \"orange\")\n","  plt.plot_date(forecast_reconc_item.ds, forecast_reconc_item.y_pred, \"o-\", label=\"reconc forecast\", color = \"green\")\n","  plt.grid()\n","  plt.legend(loc=\"best\")\n","  plt.show()\n","\n","# COMMAND ----------\n","\n","from typing import Union\n","from tqdm.notebook import tqdm_notebook as tqdm\n","\n","class WRMSSEEvaluator(object):\n","\n","    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n","        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n","        train_target_columns = train_y.columns.tolist()\n","        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n","\n","        train_df['all_id'] = 0  # for lv1 aggregation\n","\n","        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n","        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n","\n","        if not all([c in valid_df.columns for c in id_columns]):\n","            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n","\n","        self.train_df = train_df\n","        self.valid_df = valid_df\n","        self.calendar = calendar\n","        self.prices = prices\n","\n","        self.weight_columns = weight_columns\n","        self.id_columns = id_columns\n","        self.valid_target_columns = valid_target_columns\n","\n","        weight_df = self.get_weight_df()\n","\n","        self.group_ids = (\n","            'all_id',\n","            'state_id',\n","            'store_id',\n","            'cat_id',\n","            'dept_id',\n","            ['state_id', 'cat_id'],\n","            ['state_id', 'dept_id'],\n","            ['store_id', 'cat_id'],\n","            ['store_id', 'dept_id'],\n","            'item_id',\n","            ['item_id', 'state_id'],\n","            ['item_id', 'store_id']\n","        )\n","\n","        for i, group_id in enumerate(tqdm(self.group_ids)):\n","            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n","            scale = []\n","            for _, row in train_y.iterrows():\n","                series = row.values[np.argmax(row.values != 0):]\n","                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n","            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n","            setattr(self, f'lv{i + 1}_train_df', train_y)\n","            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n","\n","            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n","            print(\"LV-weight\", lv_weight)\n","            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n","            print(\"LV-weight Sum\", lv_weight.sum())\n","\n","\n","    def get_weight_df(self) -> pd.DataFrame:\n","        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n","        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n","        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n","        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n","\n","        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n","        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n","        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n","        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n","        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n","        return weight_df\n","\n","    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n","        valid_y = getattr(self, f'lv{lv}_valid_df')\n","        #print(\"Valid_y:\", valid_y)\n","        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n","        scale = getattr(self, f'lv{lv}_scale')\n","        print(\"Scale:\", scale)\n","        return (score / scale).map(np.sqrt)\n","\n","    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n","        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n","\n","        if isinstance(valid_preds, np.ndarray):\n","            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n","\n","        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n","\n","        all_scores = []\n","        for i, group_id in enumerate(self.group_ids):\n","            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n","            weight = getattr(self, f'lv{i + 1}_weight')\n","            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n","            all_scores.append(lv_scores.sum())\n","\n","        return np.mean(all_scores)\n","\n","# COMMAND ----------\n","\n","weight\n","\n","# COMMAND ----------\n","\n","train_df\n","\n","# COMMAND ----------\n","\n","day_to_week = evaluator.calendar.set_index('d')['wm_yr_wk'].to_dict()\n","weight_df = evaluator.train_df[['item_id', 'store_id'] + evaluator.weight_columns].set_index(['item_id', 'store_id'])\n","weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n","weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n","\n","# COMMAND ----------\n","\n","weight_df\n","\n","# COMMAND ----------\n","\n","weight_df = weight_df.merge(evaluator.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n","weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n","\n","# COMMAND ----------\n","\n","weight_df\n","\n","# COMMAND ----------\n","\n","weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n","weight_df\n","\n","# COMMAND ----------\n","\n","weight_df = weight_df.loc[zip(evaluator.train_df.item_id, evaluator.train_df.store_id), :].reset_index(drop=True)\n","\n","# COMMAND ----------\n","\n","weight_df\n","\n","# COMMAND ----------\n","\n","weight_df = pd.concat([evaluator.train_df[evaluator.id_columns], weight_df], axis=1, sort=False)\n","\n","# COMMAND ----------\n","\n","weight_df\n","\n","# COMMAND ----------\n","\n","weight_df\n","\n","# COMMAND ----------\n","\n","weight_df.groupby(\"store_id\")[weight_columns].sum().sum(axis = 1)\n","\n","# COMMAND ----------\n","\n","getattr(evaluator, f'lv{3}_weight')\n","\n","# COMMAND ----------\n","\n","train_y = train_df.groupby(\"store_id\")[train_df.loc[:, train_df.columns.str.startswith('d_')].columns.tolist()].sum()\n","\n","# COMMAND ----------\n","\n","for i, group_id in enumerate(evaluator.group_ids):\n","  print(group_id, i)\n","\n","# COMMAND ----------\n","\n"," valid_y = getattr(evaluator, f'lv{3}_valid_df')\n","\n","# COMMAND ----------\n","\n","getattr(evaluator, f'lv{3}_scale')\n","\n","# COMMAND ----------\n","\n","valid_y\n","\n","# COMMAND ----------\n","\n","valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum()\n","\n","# COMMAND ----------\n","\n","((valid_y - valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum())**2).mean(axis = 1)\n","\n","# COMMAND ----------\n","\n","test = ((valid_y - valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum())**2).reset_index(drop = True)\n","\n","# COMMAND ----------\n","\n","test.iloc[0].mean()\n","\n","# COMMAND ----------\n","\n","((valid_y - valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum())**2)\n","\n","# COMMAND ----------\n","\n","lv_scores = evaluator.rmsse(valid_preds.groupby('store_id')[evaluator.valid_target_columns].sum(), 3)\n","\n","# COMMAND ----------\n","\n","lv_scores\n","\n","# COMMAND ----------\n","\n","lv_scores\n","\n","# COMMAND ----------\n","\n","valid_preds = pd.concat([evaluator.valid_df[evaluator.id_columns], forecast_right_format.reset_index(drop = True)], axis=1, sort=False)\n","\n","\n","# COMMAND ----------\n","\n","valid_preds\n","\n","# COMMAND ----------\n","\n","evaluator = WRMSSEEvaluator(train_df =  new, valid_df = new2, calendar =  calendar, prices = sell_prices)\n","evaluator.score(forecast_right_format.reset_index(drop = True))\n","\n","# COMMAND ----------\n","\n","evaluator.rmsse(new2)\n","\n","# COMMAND ----------\n","\n","## Prepare data to calculate WRMSSE  ##\n","valid_df = sales_train.loc[:,\"d_1913\": \"d_1941\"]\n","train_df = sales_train.drop(sales_train.loc[:,\"d_1913\": \"d_1941\"], axis = 1)\n","train_df = train_df.drop({\"ts_id\"}, axis = 1)\n","\n","# [Formatieren für WRMSSE Berechnung]\n","help_train = train_df[[\"id\", \"store_id\", \"item_id\"]]\n","calendar_merge = calendar[[\"date\", \"d\"]].rename({\"date\": \"ds\"}, axis = 1)\n","calendar_merge[\"ds\"] = pd.to_datetime(calendar_merge[\"ds\"])\n","\n","# COMMAND ----------\n","\n","# Calculate WRMSSE for forecast without trend correction and reconciliation ##\n","forecast_id = help_train.reset_index().merge(forecast, on = [\"store_id\", \"item_id\"]).set_index('index')\n","forecast_help = forecast_id.reset_index().merge(calendar_merge, on =\"ds\").set_index(\"index\")\n","forecast_right_format = forecast_help[[\"d\", \"item_id\",\"store_id\",\"id\", \"y_pred\"]].pivot(columns =  \"d\", values = \"y_pred\")\n","new = train_df.reset_index(drop = True)\n","new2 = valid_df.reset_index(drop = True)\n","\n","# COMMAND ----------\n","\n","#valid_df: new2, calendar: pd.DataFrame, prices: pd.DataFrame):\n","train_y = new.loc[:, new.columns.str.startswith('d_')]\n","train_target_columns = train_y.columns.tolist()\n","weight_columns = train_y.iloc[:, -28:].columns.tolist()\n","\n","# COMMAND ----------\n","\n","train_df['all_id'] = 0  # for lv1 aggregation\n","\n","id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n","valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n","\n","if not all([c in valid_df.columns for c in id_columns]):\n","    valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n","\n","# COMMAND ----------\n","\n","day_to_week = calendar.set_index('d')['wm_yr_wk'].to_dict()\n","weight_df = train_df[['item_id', 'store_id'] + weight_columns].set_index(['item_id', 'store_id'])\n","weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n","weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n","\n","# COMMAND ----------\n","\n","evaluator = WRMSSEEvaluator(train_df =  new, valid_df = new2, calendar =  calendar, prices = sell_prices)\n","\n","# COMMAND ----------\n","\n","evaluator = WRMSSEEvaluator(train_df =  new, valid_df = new2, calendar =  calendar, prices = sell_prices)\n","evaluator.get_weight_df()\n","evaluator.score(forecast_right_format.reset_index(drop = True))\n","\n","# COMMAND ----------\n","\n","## Calculate WRMSSE for forecast with trend correction and reconciliation ##\n","forecast_reconc_wmrsse = train_df[[\"id\", \"store_id\", \"item_id\"]].reset_index().merge(forecast_reconc, on = \"id\").set_index(\"index\")\n","forecast_reconc_wmrsse = forecast_reconc_wmrsse[[\"ds\",\"item_id\",\"store_id\",\"id\", \"y_pred\"]].reset_index().merge(calendar_merge, on = \"ds\").set_index(\"index\")\n","\n","# Pivot table #\n","forecast_reconc_wmrsse = forecast_reconc_wmrsse[[\"d\", \"item_id\",\"store_id\",\"id\", \"y_pred\"]].pivot(columns =  \"d\", values = \"y_pred\")\n","evaluator.score(forecast_reconc_wmrsse.reset_index(drop = True))\n","\n","\n","# COMMAND ----------\n","\n","train_df[[\"id\", \"store_id\", \"item_id\"]]\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}